# Deep Text Generation ðŸ“
> Experimenting with small deep learning models for natural text generation

## ToDo
- [x] implement word-based model
- [x] implement char-based model
- [ ] try exponential LR decay 
- [ ] try subword tokenizer
- [ ] implement temperature
- [ ] try [keras mini GPT example](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)
- [x] use [German recipes](https://www.kaggle.com/sterby/german-recipes-dataset) data set for more standardized corpus
- [ ] benchmark mixed precision: `tf.keras.mixed_precision.set_global_policy("mixed_float16")`
- [ ] implement shifted-sequence model like [this](https://www.tensorflow.org/text/tutorials/text_generation) rather than manipulating data set 

# Data ðŸ’¿
WIP.

# Models ðŸ¤–
WIP.

# Results ðŸ”¬
WIP.
